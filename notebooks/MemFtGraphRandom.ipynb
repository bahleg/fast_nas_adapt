{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9879be41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "/usr/local/lib/python3.8/dist-packages/huggingface_hub/utils/_runtime.py:184: UserWarning: Pydantic is installed but cannot be imported. Please check your installation. `huggingface_hub` will default to not using Pydantic. Error message: '{e}'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.module2graph import GraphInterperterWithGamma\n",
    "from src.resnet18 import ResNet18\n",
    "import numpy as np\n",
    "\n",
    "import graphviz\n",
    "import itertools\n",
    "import copy\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "import networkx as nx\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Tuple, Dict # actually we don't need it for py>=3.9, but I have 3.8 on my laptop\n",
    "\n",
    "from src.utils import train_loop, test_loop\n",
    "#from numba import njit\n",
    "from src.cifar_data import get_dataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "651c865f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740c54e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dataset with 0-7 classes\n",
    "\n",
    "train_dl, test_dl = get_dataloaders([0,1,2,3,4,5,6,7], )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5beffa94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fffbd580eec244f98507e92c1fec943f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7331249713897705"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loop(model, test_dl,  \"cpu\", nc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adb347af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean \n",
    "\n",
    "def module_to_graph(m: torch.nn.Module):\n",
    "    graph = torch.fx.symbolic_trace(m).graph\n",
    "    named_dict = dict(m.named_modules())\n",
    "    edges = [] # (from, to)\n",
    "    grad = {'x': 0}\n",
    "    params = {'x': 0}\n",
    "    weights = {'x': 0} # node: params\n",
    "    for node in graph.nodes:\n",
    "        # no placeholder and call_mathod\n",
    "        if node.op == 'call_module':\n",
    "            n_params = 0\n",
    "            grad_1 = []\n",
    "            params_1 = []\n",
    "            for p in named_dict[node.target].parameters():\n",
    "                n_params += p.numel()\n",
    "                grad_1.append(p.grad)\n",
    "                params_1.append(p)\n",
    "            try:\n",
    "                grad[node.name] = torch.mean(torch.stack(grad_1))\n",
    "                params[node.name] = torch.mean(torch.stack(params_1))\n",
    "            except:\n",
    "                grad[node.name] = 0\n",
    "                params[node.name] = 0\n",
    "            weights[node.name] = n_params\n",
    "            assert len(node.args) == 1\n",
    "            for arg in node.args:\n",
    "                if type(arg) == torch.fx.Node:  # ignore constants\n",
    "                    edges.append((arg.name, node.name))\n",
    "        elif node.op == 'call_function':\n",
    "            for arg in node.args:\n",
    "                if type(arg) == torch.fx.Node:  # ignore constants\n",
    "                    edges.append((arg.name, node.name))\n",
    "            weights[node.name] = 0\n",
    "        elif node.op == 'output':\n",
    "            try:\n",
    "                edges.append((node.args[0][0].name, node.name))\n",
    "            except:\n",
    "                edges.append((node.args[0].name, node.name))\n",
    "            weights['output'] = 0\n",
    "            \n",
    "    return edges, {'_'.join(k.split('.')): v for k, v in weights.items()}, {'_'.join(k.split('.')): v for k, v in params.items()}, {'_'.join(k.split('.')): v for k, v in grad.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5f2a35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward for target model with gamma values for each edge.\n",
    "# means - mean values for arguments\n",
    "def forward_with_gammas(model, gammas: Dict[Tuple[str, str], torch.Tensor], \n",
    "                        means: Dict[str, torch.Tensor] = None, *torch_model_args):\n",
    "    args_iter = iter(torch_model_args)\n",
    "    env : Dict[str, Node] = {}\n",
    "    used_edges = set()\n",
    "    def load_arg(a):    \n",
    "        return torch.fx.graph.map_arg(a, lambda n: env[n.name])\n",
    "\n",
    "    def fetch_attr(target : str):\n",
    "        target_atoms = target.split('.')\n",
    "        attr_itr = model.graph\n",
    "        for i, atom in enumerate(target_atoms):\n",
    "            if not hasattr(attr_itr, atom):\n",
    "                raise RuntimeError(f\"Node referenced nonexistant target {'.'.join(target_atoms[:i])}\")\n",
    "            attr_itr = getattr(attr_itr, atom)\n",
    "        return attr_itr\n",
    "    named_modules = dict(model.named_modules())\n",
    "    for node in model.graph.nodes:\n",
    "        edges = []\n",
    "\n",
    "        if node.op in ['call_module', 'call_function', 'output']:    \n",
    "            if node.op == 'output':\n",
    "                edges = [(node.args[0][0].name, node.name)]\n",
    "            else:\n",
    "    \n",
    "                for arg in node.args:\n",
    "                    if type(arg) == torch.fx.Node:  # ignore constants\n",
    "                        edges.append((arg.name, node.name))\n",
    "                    else:\n",
    "                        edges.append(None)\n",
    "            gammas_node = [int(gammas[e])  if (e is not None) else 1 for e in edges ]\n",
    "                \n",
    "            #print (edges, gammas_node)\n",
    "        if node.op == 'placeholder':\n",
    "            result = next(args_iter) \n",
    "        elif node.op == 'get_attr':\n",
    "            result = fetch_attr(node.target)\n",
    "        elif node.op == 'call_function':\n",
    "            args = [a*g + (1.0 - g) * means[str(a0)] if str(a0) in means else a*g  for a0,a,g in zip(node.args,\n",
    "                                                                           load_arg(node.args), gammas_node)]\n",
    "            #print (len(args), len(node.args))\n",
    "            #print (node, [a for a in node.args])\n",
    "            #print (node, [a.shape for a in args])\n",
    "            result = node.target(*args, **load_arg(node.kwargs)) \n",
    "        elif node.op == 'call_method':\n",
    "            self_obj, *args = load_arg(node.args) \n",
    "            kwargs = load_arg(node.kwargs)\n",
    "            args =  [a*g + (1.0 - g) * means[str(a0)] if str(a0) in means else a*g   for a0, a,g in zip(node.args[1:], \n",
    "                                                                        args, gammas_node)]\n",
    "            result = getattr(self_obj, node.target)(*args, **kwargs)\n",
    "        elif node.op == 'call_module':\n",
    "            args = [a*g + (1.0 - g) * means[str(a0)] if str(a0) in means else a*g   for a0, a,g in zip(node.args, \n",
    "                                                                           load_arg(node.args), gammas_node)]\n",
    "            \n",
    "            result = named_modules[node.target](*args, **load_arg(node.kwargs)) \n",
    "        \n",
    "        result = result\n",
    "        for e in edges:\n",
    "            used_edges.add(e)\n",
    "    \n",
    "        if node.op == 'output':\n",
    "            \n",
    "            return result, env # currently ignorign means for output\n",
    "        #print (node.args, node.name, node.op, abs(result).sum().item())\n",
    "        env[node.name] = result\n",
    "        \n",
    "    return result\n",
    "\n",
    "# a wrapper that takes model and uses forward_with_Gammas\n",
    "class PrunedModel(torch.nn.Module):\n",
    "    def __init__(self, base, prune_dict, means = None):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "        self.prune_dict = prune_dict\n",
    "        self.means = means \n",
    "    def forward(self, x):\n",
    "        return forward_with_gammas(self.base, self.prune_dict,  self.means, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ab87901",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/legin/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([('x', 'model_conv1'),\n",
       "  ('model_conv1', 'model_bn1'),\n",
       "  ('model_bn1', 'model_relu'),\n",
       "  ('model_relu', 'model_maxpool'),\n",
       "  ('model_maxpool', 'model_layer1_0_conv1'),\n",
       "  ('model_layer1_0_conv1', 'model_layer1_0_bn1'),\n",
       "  ('model_layer1_0_bn1', 'model_layer1_0_relu'),\n",
       "  ('model_layer1_0_relu', 'model_layer1_0_conv2'),\n",
       "  ('model_layer1_0_conv2', 'model_layer1_0_bn2'),\n",
       "  ('model_layer1_0_bn2', 'add')],\n",
       " [('x', 0),\n",
       "  ('model_conv1', 9408),\n",
       "  ('model_bn1', 128),\n",
       "  ('model_relu', 0),\n",
       "  ('model_maxpool', 0),\n",
       "  ('model_layer1_0_conv1', 36864),\n",
       "  ('model_layer1_0_bn1', 128),\n",
       "  ('model_layer1_0_relu', 0),\n",
       "  ('model_layer1_0_conv2', 36864),\n",
       "  ('model_layer1_0_bn2', 128)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges, weights, a, b = module_to_graph(ResNet18())\n",
    "# edges, weights\n",
    "edges[:10], list(weights.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0e7bb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets intermediate representations of nodes\n",
    "def get_inter(model, *torch_model_args) -> dict:\n",
    "    args_iter = iter(torch_model_args)\n",
    "    env : Dict[str, Node] = {}\n",
    "    used_edges = set()\n",
    "    inter = {}\n",
    "    def load_arg(a):    \n",
    "        return torch.fx.graph.map_arg(a, lambda n: env[n.name])\n",
    "\n",
    "    def fetch_attr(target : str):\n",
    "        target_atoms = target.split('.')\n",
    "        attr_itr = model.graph\n",
    "        for i, atom in enumerate(target_atoms):\n",
    "            if not hasattr(attr_itr, atom):\n",
    "                raise RuntimeError(f\"Node referenced nonexistant target {'.'.join(target_atoms[:i])}\")\n",
    "            attr_itr = getattr(attr_itr, atom)\n",
    "        return attr_itr\n",
    "    named_modules = dict(model.named_modules())\n",
    "    for node in model.graph.nodes:\n",
    "        edges = []\n",
    "\n",
    "        if node.op in ['call_module', 'call_function', 'output']:    \n",
    "            if node.op == 'output':\n",
    "                edges = [(node.args[0][0].name, node.name)]\n",
    "            else:\n",
    "    \n",
    "                for arg in node.args:\n",
    "                    if type(arg) == torch.fx.Node:  # ignore constants\n",
    "                        edges.append((arg.name, node.name))\n",
    "                    else:\n",
    "                        edges.append(None)\n",
    "                \n",
    "            #print (edges, gammas_node)\n",
    "        if node.op == 'placeholder':\n",
    "            result = next(args_iter) \n",
    "        elif node.op == 'get_attr':\n",
    "            result = fetch_attr(node.target)\n",
    "        elif node.op == 'call_function':\n",
    "            \n",
    "            args = load_arg(node.args)\n",
    "            for a_, a in zip(node.args, args):\n",
    "                inter[a_] = a\n",
    "            #print (len(args), len(node.args))\n",
    "            #print ([a.shape for a in load_arg(node.args)], [a.shape for a in args])\n",
    "            result = node.target(*args, **load_arg(node.kwargs)) \n",
    "        elif node.op == 'call_method':\n",
    "            self_obj, *args = load_arg(node.args) \n",
    "            \n",
    "            for a_, a in zip(node.args[1:], args):\n",
    "                inter[a_] = a\n",
    "            kwargs = load_arg(node.kwargs)\n",
    "            result = getattr(self_obj, node.target)(*args, **kwargs)\n",
    "        elif node.op == 'call_module':\n",
    "            args = load_arg(node.args)\n",
    "            for a_, a in zip(node.args, args):\n",
    "                inter[a_] = a\n",
    "            result = named_modules[node.target](*args, **load_arg(node.kwargs)) \n",
    "        \n",
    "        \n",
    "        result = result\n",
    "        for e in edges:\n",
    "            used_edges.add(e)\n",
    "    \n",
    "        if node.op == 'output':\n",
    "            \n",
    "            return inter \n",
    "        #print (node.args, node.name, node.op, abs(result).sum().item())\n",
    "        env[node.name] = result\n",
    "        \n",
    "    return inter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c81d8728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/legin/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad inter 1\n",
      "bad inter 1\n",
      "bad inter 1\n",
      "bad inter 1\n",
      "bad inter 1\n",
      "bad inter 1\n",
      "bad inter 1\n",
      "bad inter 1\n",
      "bad inter 1\n",
      "bad inter 1\n",
      "bad inter 1\n",
      "bad inter 1\n",
      "bad inter 1\n",
      "bad inter 1\n",
      "bad inter 1\n",
      "bad inter 1\n"
     ]
    }
   ],
   "source": [
    "# getting subset to evaulate mean\n",
    "train_dl_limit, _ = get_dataloaders([0,1,2,3,4,5,6,7], train_limit=256)# 256\n",
    "\n",
    "# getting mean\n",
    "inter = {}\n",
    "model = ResNet18(8)\n",
    "# this is a checkpoint from master branch\n",
    "model.load_state_dict(torch.load('./model_last.ckpt', map_location='cpu'))\n",
    "tr = torch.fx.symbolic_trace(model)\n",
    "elem_count = 0\n",
    "for x,_ in train_dl_limit:\n",
    "    elem_count += x.shape[0]\n",
    "    i_ = get_inter(tr, x)\n",
    "    for k in i_:\n",
    "        try:\n",
    "            if k not in inter:\n",
    "                inter[str(k)] = i_[k].sum(0).detach()\n",
    "            else:\n",
    "                inter[str(k)] += i_[k].sum(0).detach()\n",
    "        except:\n",
    "            print ('bad inter', k)\n",
    "            #inter[str(k)] = 0.0\n",
    "    for k in inter:\n",
    "        inter[k] /= elem_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9592e369",
   "metadata": {},
   "source": [
    "<!-- ### Conclusion\n",
    "\n",
    "It seems that the problem is NP-hard. We need to come up with a new approach.\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed9a975",
   "metadata": {},
   "source": [
    "<!-- # The second attempt\n",
    "\n",
    "Consider the following heuristic\n",
    "\n",
    "1. Top sort (v_i, v_j) => i < j\n",
    "\n",
    "2. for k in {n, ..., 1}\n",
    "\n",
    "Consider 2 cases:\n",
    "\n",
    "a) put v_k into the layer of its nearest child + prune some edges\n",
    "\n",
    "b) put vk into a new layer => \n",
    "\n",
    "Consider all subsets of outcoming edges. We instantly identify a layer given a subset. So, we aggregate this layer with the answer of the nearest child to v_k\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583cac8f",
   "metadata": {},
   "source": [
    "## The second attempt + deleting of edges\n",
    "\n",
    "1. Find all (sample) topological sorts\n",
    "\n",
    "https://www.geeksforgeeks.org/all-topological-sorts-of-a-directed-acyclic-graph/\n",
    "\n",
    "2. Apply greedy dynamic programming to find a monotonous solution\n",
    "\n",
    "3. Postprocess a graph: remove all nodes that are unreacheble from \"x\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02f23f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/legin/.local/lib/python3.8/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9261"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DG = nx.DiGraph(edges)\n",
    "all_sorts = list(nx.all_topological_sorts(DG))\n",
    "len(all_sorts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec1813ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @njit\n",
    "def dp_for_top_sort(edges, weights, e_importance, top_sort_str, memory=1e10):\n",
    "    node_ids = {k: i for i, k in enumerate(weights)}\n",
    "    id_to_node = [node for _, node in enumerate(weights)]\n",
    "    top_sort = np.array([node_ids[n] for n in top_sort_str])\n",
    "    assert top_sort[-1] == node_ids['output']\n",
    "    assert top_sort[0] == node_ids['x']\n",
    "    assert top_sort.shape[0] == len(node_ids)\n",
    "    m = np.zeros((len(node_ids), len(node_ids))).astype(np.int32)\n",
    "    id_to_weight = np.array([weights[n] for n in id_to_node])\n",
    "    assert len(edges) == len(e_importance)\n",
    "    for (src, dst), w in zip(edges, e_importance):\n",
    "        src_id, dst_id = node_ids[src], node_ids[dst]\n",
    "        m[src_id, dst_id] = w\n",
    "        \n",
    "    node_to_layers = np.ones((len(node_ids), len(node_ids))).astype(np.int32) * (-100)  # ans for each v ->\n",
    "    node_to_layers[top_sort[-1], top_sort[-1]] = 0\n",
    "    dp = [1e9] * len(node_ids)\n",
    "    dp[top_sort[-1]] = 0\n",
    "    for i in range(len(node_ids) - 2, -1, -1):\n",
    "        v = top_sort[i]\n",
    "        for j in range(i, len(node_ids)):  # the last node of the first layer (starting from v)\n",
    "            if id_to_weight[top_sort[i: j + 1]].sum() > memory:\n",
    "                continue\n",
    "            if j == len(node_ids) - 1:\n",
    "                dp[v] = 0\n",
    "                node_to_layers[v, top_sort[i:]] = 0\n",
    "                continue\n",
    "            v_j = top_sort[j + 1]\n",
    "            next_layer_ids = [] if j + 1 >= len(node_ids) else \\\n",
    "            [k for k in range(m.shape[0]) if node_to_layers[v_j, k] == node_to_layers[v_j, v_j]]\n",
    "            pruned_value = sum([m[top_sort[k], l] for k in range(i, j + 1) for l in next_layer_ids if m[top_sort[k], l] != 0])\n",
    "            if dp[v_j] + pruned_value <= dp[v]:\n",
    "                dp[v] = dp[v_j] + pruned_value\n",
    "                node_to_layers[v] = node_to_layers[v_j]\n",
    "                node_to_layers[v, top_sort[i:j + 1]] = node_to_layers[v].max() + 1\n",
    "                \n",
    "    # prune restricted edges (TODO: also prune unreacheble nodes)\n",
    "    ans = node_to_layers[node_ids['x']]\n",
    "    pruned_edges_ids = [(i, j) for i in range(m.shape[0]) for j in range(m.shape[0]) \\\n",
    "                    if m[i, j] != 0 and abs(ans[i] - ans[j]) > 1]\n",
    "    pruned_edges = [(id_to_node[i], id_to_node[j]) for i, j in pruned_edges_ids]\n",
    "    pruned_value = sum([m[i, j] for i, j in pruned_edges_ids])\n",
    "    \n",
    "    reach_ids = [set() for _ in range(len(node_ids))]\n",
    "    for i in range(len(node_ids) - 1, -1, -1):\n",
    "        v = top_sort[i]\n",
    "        reach_ids[v].add(v)\n",
    "        for k in range(i + 1, m.shape[0]):\n",
    "            v_c = top_sort[k]\n",
    "            if m[v, v_c] != 0 and (v, v_c) not in pruned_edges_ids:\n",
    "                reach_ids[v] |= reach_ids[v_c]\n",
    "    conn_g = top_sort[-1] in reach_ids[top_sort[0]]\n",
    "                \n",
    "    return {'node_to_layer': {id_to_node[i]: ans.max() - l for i, l in enumerate(ans)},\n",
    "            'pruned_value': pruned_value, 'pruned_edges': pruned_edges,\n",
    "            'connected_graph': conn_g}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f0a1295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/legin/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Using cache found in /home/legin/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1366ded57f20403a9a1f88255d3d6fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9261 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/legin/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e2fa7b948f445ae9eec37878d38d26b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d959969bd44ff482e65de5922a6b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f12a30ea7225482abc30f95ffcf6c64e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/legin/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /home/legin/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af23a4ce8c5446988fc93eeac8c4d762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9261 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/legin/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c242ce150f14830abd849823821add5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2989b1ff93a64648a7de16d6360c9d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ce4246e6c04ccb990948a2908ac205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/legin/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /home/legin/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7568647fd6d47278a4f7da372a6bc45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9261 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/legin/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e1f335e50d407e98265ae8a8541b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce87174fa87f4b64953caeb92516f5c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03c430c5f16346e09b9bc8472ce1ae53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/legin/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /home/legin/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e9411afbe1a4777b1d9558a58878c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9261 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/legin/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b856be8fce434a9c858d1a74e492a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cace60a9c644def87748ad753b8b50b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c00d128ffd4b5c88b60cba4b59864a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/legin/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /home/legin/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa49ea3c312c449e876c6b679019cd14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9261 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/legin/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0da3679caa8f4f36a379f155c5bb9aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef2cec4ad784bd2aeedb876f5216c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e039462fd65a409c93e1f8c4d6e32a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/legin/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /home/legin/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a22b110b8c94c0ba467d840ac47ea32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9261 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/legin/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e160592704cf47f3ab41b34f8d85d5da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ab2bba2c354a5ead9df9b15ab34d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96c82c9da91b43b4a55f292955cbe522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/legin/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /home/legin/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0914df9cab8e44efbf83fc85b7e3ac59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9261 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/legin/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a77920a48e4f29b22eab2c758ab1c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11fef23a9f814139aa591a11e159caf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c38a074fe634224a5b259da038dfe32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/legin/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /home/legin/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1506885d02cc4ae2b14b45b4191c34ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9261 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/legin/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2d50ec997ce4550b0b7f09f58824238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b47519f7c6634bacb9ddcb46c350e51d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf0bd56994d84d3cbb7ac214b6414b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/legin/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /home/legin/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58100c9881374e13a5dd5203b02a110e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9261 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/legin/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff8e657a3b949d3a09afd1b45e1907e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3233107748c424fa85765a8937e78a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2667069a513245e1bb9b0dac52e33e68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/legin/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /home/legin/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cffeb545abb4c1a8e0b302a14a1afd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9261 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/legin/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45906495ba04019911dcb894b56742c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62051261673c451d8dfeec1b3c2598e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb51ce56f1d44b3886eabddedf4a2ae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/legin/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /home/legin/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b325a03c2ba44b68bff277e6049257e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9261 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/legin/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8702fcee8f254d3b856d36eeb57be68d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61d00c96e4914d53a0b9ea34877f326c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b5386e72c8e49868848cc380038f4dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/legin/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /home/legin/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ac110706b5f443a970a5b071ba7f40b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9261 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/legin/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea3dc5a6ab6f484ea7343b5d6e7fedb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea1b6e2145e4262856130cef39dc652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1792e7b71e464f24a49ff7768b49785d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# iterates by 3 seeds and edge_mem from 2 to 6\n",
    "# during best architecture selection takes random value as a criterion\n",
    "# fine-tunes and evaluates\n",
    "eval_dict = {}\n",
    "fine_dict = {} \n",
    "\n",
    "for attemp in range(3):\n",
    "    for edge_mem in range(2, 6):\n",
    "        \n",
    "        model = ResNet18(8)\n",
    "        model.load_state_dict(torch.load('./model_last.ckpt', map_location='cpu'))\n",
    "        warp = GraphInterperterWithGamma(model)\n",
    "\n",
    "        named_dict = dict(model.named_modules())\n",
    "\n",
    "\n",
    "        edges, weights, a, b = module_to_graph(ResNet18())\n",
    "        # edges, weights\n",
    "        edges[:10], list(weights.items())[:10]\n",
    "        edges_importance = [1]*len(edges)\n",
    "\n",
    "        best_pruned = 1e10\n",
    "        best_val = None\n",
    "        for s in tqdm(all_sorts):\n",
    "            # for mem=8 the computation takes time\n",
    "            res = dp_for_top_sort(edges, {k: 1 for k in weights}, edges_importance, s, edge_mem)  \n",
    "            res['pruned_value'] = np.random.rand()\n",
    "            if res['connected_graph'] == True and best_pruned > res['pruned_value']:\n",
    "                best_pruned = res['pruned_value']\n",
    "                best_val = res\n",
    "\n",
    "            if best_pruned == 0:\n",
    "                print(res)\n",
    "                break\n",
    "\n",
    "\n",
    "        model = ResNet18(8)\n",
    "        model.load_state_dict(torch.load('./model_last.ckpt', map_location='cpu'))\n",
    "\n",
    "        wrapped = torch.fx.symbolic_trace(model)\n",
    "        pruned = PrunedModel(wrapped, {k:1.0 if k not in best_val['pruned_edges'] else 0.0 for k in edges },\n",
    "                             inter)\n",
    "\n",
    "        res = test_loop(pruned, test_dl,  \"cpu\", nc=8)\n",
    "        if edge_mem not in eval_dict:\n",
    "            eval_dict[edge_mem] = []\n",
    "        eval_dict[edge_mem].append(res)\n",
    "\n",
    "        train_loop(pruned, train_dl, test_dl, 9999999999, 1, 1e-3,  \"cpu\")\n",
    "        res = test_loop(pruned, test_dl,  \"cpu\", nc=8)\n",
    "        if edge_mem not in fine_dict:\n",
    "            fine_dict[edge_mem] = []\n",
    "        fine_dict[edge_mem].append(res)\n",
    "\n",
    "        import pickle\n",
    "        with open('random_mean.pckl', 'wb') as out:\n",
    "            out.write(pickle.dumps([eval_dict, fine_dict]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b9a69a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{2: [0.125, 0.12600000202655792, 0.125],\n",
       "  3: [0.125, 0.125, 0.125],\n",
       "  4: [0.12524999678134918, 0.12524999678134918, 0.125],\n",
       "  5: [0.1368750035762787, 0.125, 0.21812500059604645]},\n",
       " {2: [0.5640000104904175, 0.5180000066757202, 0.5730000138282776],\n",
       "  3: [0.5625, 0.5435000061988831, 0.5742499828338623],\n",
       "  4: [0.6353750228881836, 0.6573749780654907, 0.6784999966621399],\n",
       "  5: [0.6420000195503235, 0.6677500009536743, 0.6775000095367432]}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results using zero as mean\n",
    "import pickle\n",
    "with open('./randn.pckl', 'rb') as inp:\n",
    "    res = pickle.loads(inp.read())\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ee91dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{2: [0.12962499260902405, 0.125, 0.125],\n",
       "  3: [0.125, 0.125, 0.125],\n",
       "  4: [0.12612499296665192, 0.12612499296665192, 0.125],\n",
       "  5: [0.12587499618530273, 0.12587499618530273, 0.12562499940395355]},\n",
       " {2: [0.5892500281333923, 0.5241249799728394, 0.6132500171661377],\n",
       "  3: [0.5706250071525574, 0.6041250228881836, 0.5644999742507935],\n",
       "  4: [0.6047499775886536, 0.6507499814033508, 0.6359999775886536],\n",
       "  5: [0.6508749723434448, 0.6942499876022339, 0.6966249942779541]}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results with estimated mean\n",
    "import pickle\n",
    "with open('./random_mean.pckl', 'rb') as inp:\n",
    "    res = pickle.loads(inp.read())\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
